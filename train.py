import torch
import torch.nn as nn
import torch.nn.functional as F
from PFSPNet import PFSPNet
from critic_network import CriticNetwork
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from calc.calculate_objectives import calculate_objectives_pytorch
from calc.pareto_archive import ParetoArchive

# ==============================================================================
# MODIFIED train_one_batch function
# å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ç›®æ ‡å‡½æ•°è®¡ç®— Cmaxï¼Œå¹¶ä¼ å…¥æ›´å¤æ‚çš„é—®é¢˜å‚æ•°
# ==============================================================================
def train_one_batch(
    actor_model, critic_model, optimizer_actor, optimizer_critic,
    batch_instance_features,
    batch_num_machines_scalar,
    # === æ–°å¢ï¼šä¼ å…¥æƒé‡ ===
    w_cmax, w_tec,
    # ========================
    P_instance, E_instance, R_instance, u_instance, s_instance, f_instance,
    device='cuda' if torch.cuda.is_available() else 'cpu'
):
    batch_size = batch_instance_features.size(0)
    num_total_jobs = batch_instance_features.size(1)

    actor_model.train()
    critic_model.train()

    batch_instance_features = batch_instance_features.to(device)
    batch_num_machines_scalar = batch_num_machines_scalar.to(device)

    # Actor æ‰§è¡Œ (é€»è¾‘ä¸å˜)
    selected_indices, log_probs_distributions, encoded_jobs = actor_model(
        batch_instance_processing_times=batch_instance_features,
        batch_num_machines_scalar=batch_num_machines_scalar,
        max_decode_len=num_total_jobs
    )

    # --- è®¡ç®—åŸå§‹ç›®æ ‡å€¼ ---
    put_off_matrices_zeros = torch.zeros(batch_size, P_instance.size(1), num_total_jobs, device=device, dtype=torch.long)
    actual_cmax_tensor, actual_tec_tensor = calculate_objectives_pytorch(
        job_sequences=selected_indices,
        put_off_matrices=put_off_matrices_zeros,
        P=P_instance, E=E_instance, R=R_instance, u=u_instance, s=s_instance, f=f_instance,
        device=device
    )
    
    # (é‡è¦) å¤„ç†æ— æ•ˆè§£
    actual_cmax_tensor[torch.isinf(actual_cmax_tensor)] = 5000.0
    actual_tec_tensor[torch.isinf(actual_tec_tensor)] = 50000.0
    
    
    # --- å¤„ç†é‡çº²é—®é¢˜å¹¶åˆå¹¶ç›®æ ‡ ---
    # 1. å¯¹ Cmax å’Œ TEC è¿›è¡Œæ‰¹æ¬¡å½’ä¸€åŒ– (å‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®)
    #    ä¸ºé˜²æ­¢æ ‡å‡†å·®ä¸º0(å¦‚æœæ‰¹æ¬¡ä¸­æ‰€æœ‰å€¼éƒ½ä¸€æ ·), åœ¨åˆ†æ¯ä¸ŠåŠ ä¸€ä¸ªå¾ˆå°çš„æ•° epsilon
    # cmax_mean = actual_cmax_tensor.mean()
    # cmax_std = actual_cmax_tensor.std()
    # tec_mean = actual_tec_tensor.mean()
    # tec_std = actual_tec_tensor.std()
    
    # # åªæœ‰å½“æ‰¹æ¬¡å¤§å°å¤§äº1ä¸”å€¼ä¸å…¨ç›¸åŒæ—¶ï¼Œstdæ‰ä¸ä¸º0ï¼Œæ‰è¿›è¡Œå½’ä¸€åŒ–
    # if cmax_std > 1e-6:
    #     normalized_cmax = (actual_cmax_tensor - cmax_mean) / (cmax_std + 1e-8)
    # else:
    #     normalized_cmax = torch.zeros_like(actual_cmax_tensor) # å¦‚æœå€¼éƒ½ä¸€æ ·ï¼Œå½’ä¸€åŒ–åä¸º0

    # if tec_std > 1e-6:
    #     normalized_tec = (actual_tec_tensor - tec_mean) / (tec_std + 1e-8)
    # else:
    #     normalized_tec = torch.zeros_like(actual_tec_tensor)
        
    # # 2. ä½¿ç”¨é¢„è®¾æƒé‡è®¡ç®—åŠ æƒç»„åˆç›®æ ‡
    # #    è¿™ä¸ª combined_objective å°†ä½œä¸ºæˆ‘ä»¬æ–°çš„å¥–åŠ±ä¿¡å·
    # combined_objective = (w_cmax * normalized_cmax + w_tec * normalized_tec)
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šç”¨â€œé™æ€ç¼©æ”¾â€æ›¿ä»£â€œæ‰¹æ¬¡å½’ä¸€åŒ–â€ ---
    # 1. å®šä¹‰ç¼©æ”¾å› å­ (ä½ å¯ä»¥æ ¹æ®ç›®æ ‡çš„å®é™…èŒƒå›´è°ƒæ•´)
    #    ç›®æ ‡æ˜¯è®©ç¼©æ”¾åçš„ cmax å’Œ tec åœ¨å·®ä¸å¤šçš„æ•°é‡çº§ä¸Š
    CMAX_SCALE_FACTOR = 200.0  # e.g., Cmax=400 -> 1.0
    TEC_SCALE_FACTOR = 8000.0  # e.g., TEC=9000 -> 1.0

    # 2. è®¡ç®—ç¼©æ”¾åçš„ç›®æ ‡å€¼
    #    æ³¨æ„æˆ‘ä»¬è¿™é‡Œæ˜¯ç›´æ¥ç”¨åŸå§‹å€¼ç›¸é™¤ï¼Œç›®æ ‡æ˜¯é™ä½å…¶æ•°å€¼ï¼Œä½¿å…¶å…·æœ‰å¯æ¯”æ€§
    #    æˆ‘ä»¬å¸Œæœ›æ¨¡å‹æœ€å°åŒ–è¿™ä¸¤ä¸ªå€¼
    scaled_cmax = actual_cmax_tensor / CMAX_SCALE_FACTOR
    scaled_tec = actual_tec_tensor / TEC_SCALE_FACTOR

    # 3. ä½¿ç”¨é¢„è®¾æƒé‡è®¡ç®—ç»„åˆç›®æ ‡
    #    è¿™ä¸ª combined_objective å°†ä½œä¸ºæˆ‘ä»¬æ–°çš„å¥–åŠ±ä¿¡å·
    #    æ³¨æ„è¿™é‡Œæˆ‘ä»¬æ˜¯åœ¨æœ€å°åŒ–ï¼Œæ‰€ä»¥å¥–åŠ±åº”è¯¥æ˜¯è´Ÿçš„ç›®æ ‡å€¼
    combined_objective = - (w_cmax * scaled_cmax + w_tec * scaled_tec)
    
    
    # Critic æ‰§è¡Œä¸æ›´æ–° (ä½¿ç”¨ç»„åˆç›®æ ‡)
    baseline_estimates_b = critic_model(encoded_jobs).squeeze(-1)
    # Critic åº”è¯¥é¢„æµ‹ç»„åˆç›®æ ‡çš„æœŸæœ›å€¼ï¼Œè€Œä¸æ˜¯å•ä¸€ç›®æ ‡
    critic_loss = F.mse_loss(baseline_estimates_b, combined_objective.detach())
    optimizer_critic.zero_grad()
    critic_loss.backward(retain_graph=True)
    optimizer_critic.step()

    # Actor æ›´æ–° (ä½¿ç”¨ç»„åˆç›®æ ‡)
    # ä¼˜åŠ¿ç°åœ¨ä¹ŸåŸºäºç»„åˆç›®æ ‡
    advantage = combined_objective - baseline_estimates_b.detach()
    gathered_log_probs = torch.gather(log_probs_distributions, 2, selected_indices.unsqueeze(2)).squeeze(2)
    sum_log_probs_for_sequence = gathered_log_probs.sum(dim=1)
    actor_loss = (-advantage * sum_log_probs_for_sequence).mean()
    
    optimizer_actor.zero_grad()
    actor_loss.backward()
    optimizer_actor.step()

    # --- ä¿®æ”¹: è¿”å›ä¸¤ä¸ªåŸå§‹ç›®æ ‡å€¼ä»¥ä¾›è®°å½• ---
    # return actor_loss.item(), critic_loss.item(), actual_cmax_tensor.mean().item(), actual_tec_tensor.mean().item()
    
    return actor_loss.item(), critic_loss.item(), actual_cmax_tensor, actual_tec_tensor



if __name__ == '__main__':

    seed = 42
    torch.manual_seed(seed)
    np.random.seed(seed)
    print(f"å·²ä¸º PyTorch å’Œ NumPy è®¾ç½®éšæœºç§å­: {seed}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # --- å®šä¹‰é€šç”¨å‚æ•° ---
    config_batch_size = 16
    config_num_jobs = 20
    config_num_machines = 5
    config_k_intervals = 10
    config_epochs = 100
    config_batches_per_epoch = 50
    # --- æ–°å¢: å®šä¹‰ç›®æ ‡æƒé‡ ---
    # ä½ å¯ä»¥è°ƒæ•´è¿™ä¸¤ä¸ªæƒé‡æ¥æ¢ç´¢ä¸åŒçš„åå¥½
    # ä¾‹å¦‚ w_cmax=0.8, w_tec=0.2 ä¼šæ›´ä¾§é‡äºä¼˜åŒ– Cmax
    
    # !
    # config_weight_cmax = 0.5
    # config_weight_tec = 0.5
    # print(f"ç›®æ ‡æƒé‡: Cmax = {config_weight_cmax}, TEC = {config_weight_tec}")


    # --- æ–°å¢ï¼šå®šä¹‰ä¸€ä¸ªå›ºå®šçš„é—®é¢˜å®ä¾‹ï¼Œç”¨äºæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ ---
    P_instance = torch.rand(config_num_jobs, config_num_machines, device=device) * 20 + 1
    E_instance = torch.rand(config_num_jobs, config_num_machines, device=device) * 10
    R_instance = torch.randint(0, 50, (config_num_jobs,), device=device, dtype=torch.float)
    u_starts = torch.arange(0, 200 * config_k_intervals, 200, device=device, dtype=torch.float)
    s_durations = torch.full((config_k_intervals,), 200, device=device)
    f_factors = torch.randint(1, 6, (config_k_intervals,), device=device, dtype=torch.float)

    # --- ç½‘ç»œå‚æ•° (ä¸ä½ åŸæ¥çš„ä»£ç ä¸€è‡´) ---
    NEW_SCALAR_INPUT_DIM = 2 
    enc_part1_args = {'scalar_input_dim': NEW_SCALAR_INPUT_DIM, 'embedding_dim': 32, 'hidden_dim': 64, 'rnn_type': 'RNN', 'num_rnn_layers':1}
    enc_part2_args = {'p_vector_dim': enc_part1_args['hidden_dim'], 'm_embedding_dim': 16, 'output_dim': 48}
    ENC_OUT_CHANNELS = 96
    enc_part3_args = {'p_tilde_dim': enc_part2_args['output_dim'], 'conv_out_channels': ENC_OUT_CHANNELS, 'conv_kernel_size': 3, 'conv_padding': 'same'}
    encoder_config_args = {'part1_args': enc_part1_args, 'part2_args': enc_part2_args, 'part3_args': enc_part3_args}
    dec_step1_pt_enc_args = {'scalar_input_dim': NEW_SCALAR_INPUT_DIM, 'embedding_dim': 30, 'hidden_dim': 50, 'rnn_type': 'RNN', 'num_rnn_layers': 1}
    decoder_config_args = {
        'step1_pt_encoder_args': dec_step1_pt_enc_args,
        'step1_m_embedding_dim': 20, 'step1_di_output_dim': 40, 'step1_fc_hidden_dims': [35],
        'step2_rnn2_hidden_dim': 70, 'step2_rnn_type': 'LSTM', 'step2_num_rnn_layers': 1,
        'attention_job_encoding_dim': ENC_OUT_CHANNELS,
        'attention_hidden_dim': 55,
        'ptr_dim': 25
    }
    critic_config_args = {
        'encoder_output_dim': ENC_OUT_CHANNELS,
        'conv_channels_list': [64, 128, 64],
        'conv_kernel_sizes_list': [3, 3, 3],
        'final_fc_hidden_dims': [32]
    }

    actor = PFSPNet(encoder_args=encoder_config_args, decoder_args=decoder_config_args).to(device)
    critic = CriticNetwork(**critic_config_args).to(device)
    # !
    opt_actor = optim.Adam(actor.parameters(), lr=1e-5)
    opt_critic = optim.Adam(critic.parameters(), lr=1e-3)

    print("æ¨¡å‹å’Œä¼˜åŒ–å™¨å®ä¾‹åŒ–å®Œæˆã€‚å¼€å§‹è®­ç»ƒå¾ªç¯...")
    
    # --- ä¿®æ”¹: å¢åŠ ç”¨äºå­˜å‚¨ Cmax æ•°æ®çš„åˆ—è¡¨ ---
    # history_epoch_avg_actor_loss = []
    # history_epoch_avg_critic_loss = []
    # history_epoch_avg_cmax = []
    # history_epoch_best_cmax = []
    # history_epoch_avg_tec = []
    # history_epoch_best_tec = []
    
    # instance_features = torch.stack([P_instance, E_instance], dim=-1)
    # batch_features = instance_features.unsqueeze(0).expand(config_batch_size, -1, -1, -1)
    # dummy_m_scalar = torch.full((config_batch_size, 1), float(config_num_machines), device=device)

    # global_best_cmax = float('inf')
    # global_best_tec = float('inf')
    # global_best_sequence_for_cmax = None
    # global_best_sequence_for_tec = None
    
    
    # # --- è®­ç»ƒå¾ªç¯ ---
    # for epoch in range(config_epochs):
    #     actor.train()
    #     critic.train()
    #     batch_actor_losses, batch_critic_losses, batch_cmaxes, batch_teces = [], [], [], []
    #     epoch_best_cmax_this_epoch = float('inf')
    #     epoch_best_tec_this_epoch = float('inf')

    #     for batch_idx in range(config_batches_per_epoch):
    #         # --- ä¿®æ”¹: å°†æƒé‡ä¼ å…¥è®­ç»ƒå‡½æ•° ---
    #         actor_loss_val, critic_loss_val, batch_avg_cmax_val, batch_avg_tec_val = train_one_batch(
    #             actor, critic, opt_actor, opt_critic,
    #             batch_features, dummy_m_scalar,
    #             config_weight_cmax, config_weight_tec, # <--- ä¼ å…¥æƒé‡
    #             P_instance, E_instance, R_instance, u_starts, s_durations, f_factors,
    #             device
    #         )
            
    #         batch_actor_losses.append(actor_loss_val)
    #         batch_critic_losses.append(critic_loss_val)
    #         # --- ä¿®æ”¹: åŒæ—¶è®°å½• Cmax å’Œ TEC ---
    #         batch_cmaxes.append(batch_avg_cmax_val)
    #         batch_teces.append(batch_avg_tec_val)

    #     # --- ä¿®æ”¹ï¼šåœ¨ Epoch ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ï¼Œæ‰¾åˆ°æœ€ä½³ Cmax å’Œ TEC ---
    #     actor.eval()
    #     with torch.no_grad():
    #         # ç”Ÿæˆä¸€æ‰¹è§£ç”¨äºè¯„ä¼°
    #         eval_batch_size = 128 # å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡æ¥åšè¯„ä¼°
    #         eval_features = instance_features.unsqueeze(0).expand(eval_batch_size, -1, -1, -1)
    #         eval_m_scalar = torch.full((eval_batch_size, 1), float(config_num_machines), device=device)
            
    #         current_batch_selected_indices, _, _ = actor(eval_features, eval_m_scalar, max_decode_len=config_num_jobs)
    #         put_off_eval = torch.zeros(eval_batch_size, config_num_machines, config_num_jobs, device=device, dtype=torch.long)
            
    #         cmax_vals, tec_vals = calculate_objectives_pytorch(
    #              current_batch_selected_indices, put_off_eval,
    #              P_instance, E_instance, R_instance, u_starts, s_durations, f_factors, device
    #         )

    #         # æ‰¾åˆ°è¿™æ‰¹è¯„ä¼°ä¸­æœ€å¥½çš„ Cmax å’Œ TEC
    #         # min_cmax_in_batch_val = torch.min(cmax_vals[torch.isfinite(cmax_vals)])
    #         # min_tec_in_batch_val = torch.min(tec_vals[torch.isfinite(tec_vals)])
            
    #         # æ‰¾åˆ°å½“å‰æ‰¹æ¬¡çš„æœ€ä¼˜å€¼å’Œå¯¹åº”çš„åºåˆ—
    #         min_cmax_in_batch_val, min_cmax_idx = torch.min(cmax_vals, dim=0)
    #         min_tec_in_batch_val, min_tec_idx = torch.min(tec_vals, dim=0)
            
            
    #         epoch_best_cmax_this_epoch = min_cmax_in_batch_val.item()
    #         epoch_best_tec_this_epoch = min_tec_in_batch_val.item()
            
    #         # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ£€æŸ¥å¹¶æ›´æ–°å†å²æœ€ä¼˜è§£ ---
    #         if min_cmax_in_batch_val.item() < global_best_cmax:
    #             global_best_cmax = min_cmax_in_batch_val.item()
    #             # ä¿å­˜è·å¾—è¿™ä¸ªæœ€ä¼˜Cmaxçš„è§£åºåˆ—
    #             global_best_sequence_for_cmax = current_batch_selected_indices[min_cmax_idx].tolist()
    #             print(f"ğŸ‰ New Best Cmax Found: {global_best_cmax:.2f}")

    #         if min_tec_in_batch_val.item() < global_best_tec:
    #             global_best_tec = min_tec_in_batch_val.item()
    #             # ä¿å­˜è·å¾—è¿™ä¸ªæœ€ä¼˜TECçš„è§£åºåˆ—
    #             global_best_sequence_for_tec = min_tec_in_batch_val.tolist()
    #             print(f"ğŸ‰ New Best TEC Found: {global_best_tec:.2f}")

    #     # Epoch æ€»ç»“
    #     avg_epoch_actor_loss = np.mean(batch_actor_losses)
    #     avg_epoch_critic_loss = np.mean(batch_critic_losses)
    #     avg_epoch_cmax = np.mean(batch_cmaxes)
    #     avg_epoch_tec = np.mean(batch_teces)

    #     history_epoch_avg_actor_loss.append(avg_epoch_actor_loss)
    #     history_epoch_avg_critic_loss.append(avg_epoch_critic_loss)
    #     history_epoch_avg_cmax.append(avg_epoch_cmax)
    #     history_epoch_best_cmax.append(epoch_best_cmax_this_epoch)
    #     history_epoch_avg_tec.append(avg_epoch_tec)
    #     history_epoch_best_tec.append(epoch_best_tec_this_epoch)
    
    #     print(f"--- Epoch {epoch+1}/{config_epochs} æ€»ç»“ ---")
    #     print(f"  æŸå¤±: Actor={avg_epoch_actor_loss:.4f}, Critic={avg_epoch_critic_loss:.4f}")
    #     print(f"  å¹³å‡ç›®æ ‡å€¼: Cmax={avg_epoch_cmax:.2f}, TEC={avg_epoch_tec:.2f}")
    #     print(f"  æœ¬ Epoch æœ€ä¼˜: Cmax={epoch_best_cmax_this_epoch:.2f}, TEC={epoch_best_tec_this_epoch:.2f}\n")

    # print("è®­ç»ƒå®Œæˆ!")

    # # --- ä¿®æ”¹: ç»˜åˆ¶ Cmax å’Œ TEC çš„åŒç›®æ ‡å›¾ä»¥åŠæŸå¤±å›¾ ---
    # epochs_range = range(1, config_epochs + 1)
    # plt.figure(figsize=(20, 6))

    # # å›¾1: Cmax éš Epoch å˜åŒ–
    # plt.subplot(1, 3, 1)
    # plt.plot(epochs_range, history_epoch_avg_cmax, label='Avg Cmax per Epoch', marker='.', color='royalblue')
    # plt.plot(epochs_range, history_epoch_best_cmax, label='Best Cmax in Epoch', marker='x', linestyle='--', color='deepskyblue')
    # plt.xlabel("Epoch")
    # plt.ylabel("Cmax")
    # plt.title("Cmax Performance over Epochs")
    # plt.legend()
    # plt.grid(True)
    
    # # å›¾2: TEC éš Epoch å˜åŒ–
    # plt.subplot(1, 3, 2)
    # plt.plot(epochs_range, history_epoch_avg_tec, label='Avg TEC per Epoch', marker='.', color='darkorange')
    # plt.plot(epochs_range, history_epoch_best_tec, label='Best TEC in Epoch', marker='x', linestyle='--', color='gold')
    # plt.xlabel("Epoch")
    # plt.ylabel("TEC")
    # plt.title("TEC Performance over Epochs")
    # plt.legend()
    # plt.grid(True)

    # # å›¾3: æŸå¤±å‡½æ•°éš Epoch å˜åŒ–
    # plt.subplot(1, 3, 3)
    # plt.plot(epochs_range, history_epoch_avg_actor_loss, label='Avg Actor Loss', marker='.', color='red')
    # plt.plot(epochs_range, history_epoch_avg_critic_loss, label='Avg Critic Loss', marker='.', color='green', alpha=0.7)
    # plt.xlabel("Epoch")
    # plt.ylabel("Loss")
    # plt.title("Losses per Epoch")
    # plt.legend()
    # plt.grid(True)
    
    # plt.tight_layout()
    # plt.show()
    
    # print("\n--- Training Finished ---")
    # print(f"Global Best Cmax: {global_best_cmax:.2f}")
    # print(f"Found with sequence: {global_best_sequence_for_cmax}")
    # print(f"Global Best TEC: {global_best_tec:.2f}")
    # print(f"Found with sequence: {global_best_sequence_for_tec}")
    
    # 1. å®ä¾‹åŒ–å¸•ç´¯æ‰˜å­˜æ¡£
    pareto_archive = ParetoArchive(capacity=100) # å­˜æ¡£å®¹é‡å¯è‡ªè¡Œè°ƒæ•´

    history_epoch_avg_actor_loss = []
    history_epoch_avg_critic_loss = []
    history_archive_size = []
    
    instance_features = torch.stack([P_instance, E_instance], dim=-1)
    batch_features = instance_features.unsqueeze(0).expand(config_batch_size, -1, -1, -1)
    dummy_m_scalar = torch.full((config_batch_size, 1), float(config_num_machines), device=device)

    # --- æ–°çš„è®­ç»ƒå¾ªç¯ ---
    for epoch in range(config_epochs):
        actor.train()
        critic.train()
        batch_actor_losses, batch_critic_losses = [], []

        for batch_idx in range(config_batches_per_epoch):
            # 2. åŠ¨æ€é‡‡æ ·æƒé‡
            w_cmax = np.random.rand()
            w_tec = 1.0 - w_cmax
            
            # 3. æ‰§è¡Œå•æ¬¡è®­ç»ƒå¹¶è·å–æ•´æ‰¹æ¬¡çš„ç›®æ ‡å€¼
            actor_loss_val, critic_loss_val, cmax_tensor, tec_tensor = train_one_batch(
                actor, critic, opt_actor, opt_critic,
                batch_features, dummy_m_scalar,
                w_cmax, w_tec, # ä¼ å…¥åŠ¨æ€æƒé‡
                P_instance, E_instance, R_instance, u_starts, s_durations, f_factors,
                device
            )
            
            batch_actor_losses.append(actor_loss_val)
            batch_critic_losses.append(critic_loss_val)
            
            # 4. å°†æ‰¹æ¬¡ä¸­æ‰€æœ‰æœ‰æ•ˆè§£å°è¯•æ·»åŠ åˆ°å­˜æ¡£
            sequences = actor.last_selected_indices
            for i in range(cmax_tensor.size(0)):
                cmax_val = cmax_tensor[i].item()
                tec_val = tec_tensor[i].item()
                # ç¡®ä¿è§£æ˜¯æœ‰æ•ˆçš„
                if np.isfinite(cmax_val) and np.isfinite(tec_val):
                    pareto_archive.add([cmax_val, tec_val], sequences[i].tolist())

        # --- Epoch æ€»ç»“ ---
        avg_epoch_actor_loss = np.mean(batch_actor_losses) if batch_actor_losses else 0
        avg_epoch_critic_loss = np.mean(batch_critic_losses) if batch_critic_losses else 0
        
        history_epoch_avg_actor_loss.append(avg_epoch_actor_loss)
        history_epoch_avg_critic_loss.append(avg_epoch_critic_loss)
        history_archive_size.append(len(pareto_archive.solutions))
   
        print(f"--- Epoch {epoch+1}/{config_epochs} æ€»ç»“ ---")
        print(f"  å­˜æ¡£å¤§å° (Archive Size): {len(pareto_archive.solutions)}")
        print(f"  å¹³å‡æŸå¤±: Actor={avg_epoch_actor_loss:.4f}, Critic={avg_epoch_critic_loss:.4f}\n")

    print("è®­ç»ƒå®Œæˆ!")

    # --- è®­ç»ƒåè¯„ä¼°ä¸ç»˜å›¾ ---
    # æœ€ç»ˆç»“æœå°±æ˜¯å¸•ç´¯æ‰˜å­˜æ¡£
    final_solutions = pareto_archive.solutions
    final_sequences = pareto_archive.sequences

    # 1. ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿å›¾
    plt.figure(figsize=(18, 6))
    plt.subplot(1, 2, 1)
    if final_solutions:
        front = np.array(final_solutions)
        # æŒ‰Cmaxæ’åºä»¥è·å¾—æ›´ç¾è§‚çš„è¿çº¿å›¾
        front = front[front[:, 0].argsort()]
        plt.scatter(front[:, 0], front[:, 1], c='red', zorder=2, label='Pareto Solutions')
        plt.plot(front[:, 0], front[:, 1], '--', c='blue', zorder=1, label='Pareto Front')
    plt.xlabel("Cmax (Makespan)")
    plt.ylabel("TEC (Total Energy Cost)")
    plt.title(f"Final Pareto Front (Size: {len(final_solutions)})")
    plt.legend()
    plt.grid(True)

    # 2. ç»˜åˆ¶æŸå¤±å’Œå­˜æ¡£å¤§å°å˜åŒ–å›¾
    plt.subplot(1, 2, 2)
    epochs_range = range(1, config_epochs + 1)
    ax1 = plt.gca()
    ax2 = ax1.twinx()
    
    p1, = ax1.plot(epochs_range, history_epoch_avg_actor_loss, 'r-', label='Actor Loss')
    p2, = ax1.plot(epochs_range, history_epoch_avg_critic_loss, 'g-', label='Critic Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss', color='black')
    ax1.tick_params(axis='y', labelcolor='black')

    p3, = ax2.plot(epochs_range, history_archive_size, 'b-', label='Archive Size')
    ax2.set_ylabel('Archive Size', color='blue')
    ax2.tick_params(axis='y', labelcolor='blue')

    plt.title('Losses and Archive Size over Epochs')
    ax1.legend(handles=[p1, p2, p3], loc='best')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

    # 3. æ‰“å°éƒ¨åˆ†æœ€ä¼˜è§£
    print("\n--- æ‰¾åˆ°çš„éƒ¨åˆ†å¸•ç´¯æ‰˜æœ€ä¼˜è§£ ---")
    # ä¸ºäº†æ–¹ä¾¿æŸ¥çœ‹ï¼ŒæŒ‰ Cmax æ’åº
    sorted_solutions = sorted(zip(final_solutions, final_sequences), key=lambda x: x[0][0])
    for i, (sol, seq) in enumerate(sorted_solutions):
        if i < 10 or i > len(sorted_solutions) - 10: # åªæ‰“å°å‰10å’Œå10ä¸ª
            print(f"  è§£ {i+1}: Cmax={sol[0]:.2f}, TEC={sol[1]:.2f}") # , Sequence={seq[:10]}...")
